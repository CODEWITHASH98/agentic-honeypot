# ðŸ§  Design Decisions

This document explains the key architectural and technology choices made in this project, including alternatives considered and rationale for final decisions.

---

## 1. Web Framework

### âœ… Chosen: **FastAPI**

| Aspect | FastAPI | Flask | Django |
|--------|---------|-------|--------|
| **Async Support** | Native | Requires Quart | Limited |
| **Performance** | Excellent | Good | Good |
| **Auto-docs** | Swagger UI built-in | Manual | DRF needed |
| **Type Safety** | Pydantic validation | Manual | Serializers |
| **Learning Curve** | Low | Low | High |

**Why FastAPI?**
- Native `async/await` for parallel Detection + Extraction
- Automatic OpenAPI documentation (`/docs`)
- Pydantic models ensure request/response validation
- Perfect for hackathon: fast to build, impressive to demo

**Why NOT Flask?**
- No native async support (would need Quart)
- No auto-generated API docs

**Why NOT Django?**
- Overkill for an API-only project
- Higher complexity, slower iteration

---

## 2. LLM Provider

### âœ… Chosen: **Groq (Llama 3.3 70B)**

| Aspect | Groq | OpenAI | Local Ollama |
|--------|------|--------|--------------|
| **Speed** | ~500ms | ~1-2s | ~5-10s |
| **Cost** | Free tier | $0.002/1K | Free |
| **Quality** | Excellent | Best | Good |
| **Reliability** | High | High | Depends |

**Why Groq?**
- **Inference speed**: 10x faster than OpenAI (critical for <2s latency goal)
- **Free tier**: Perfect for hackathons
- **Llama 3.3 70B**: State-of-the-art open model

**Why NOT OpenAI?**
- Higher latency (~1-2s per call)
- Costs money (bad for hackathon budget)
- API key more restrictive

**Why NOT Local (Ollama)?**
- Unpredictable latency
- Requires GPU setup
- Not production-deployable

---

## 3. Detection Architecture

### âœ… Chosen: **4-Tier Pipeline**

```
Tier 1 (Rules) â†’ Tier 2 (Dataset) â†’ Tier 2.5 (URL) â†’ Tier 3 (LLM) â†’ Tier 4 (Validator)
```

| Tier | Latency | Purpose |
|------|---------|---------|
| 1: Rules | <5ms | Fast keyword/pattern matching |
| 2: Dataset | <10ms | Known scam hash lookup |
| 2.5: URL | <10ms | Phishing domain detection |
| 3: LLM | ~500ms | Context understanding |
| 4: Validator | ~500ms | Self-correction |

**Why Multi-Tier?**
- **Accuracy**: Each tier catches different scam types
- **Efficiency**: Only uncertain cases go to LLM
- **Cost**: Reduces LLM API calls by ~70%
- **Self-Correction**: Tier 4 fixes LLM hallucinations

**Why NOT Single LLM?**
- Too slow for every request
- Expensive at scale
- No self-correction mechanism

**Why NOT Rules Only?**
- Misses context-dependent scams
- High false positive rate

---

## 4. State Management

### âœ… Chosen: **Redis with In-Memory Fallback**

| Aspect | Redis | PostgreSQL | In-Memory |
|--------|-------|------------|-----------|
| **Latency** | <1ms | ~5-20ms | <0.1ms |
| **Persistence** | Yes | Yes | No |
| **Scalability** | Horizontal | Vertical | None |
| **TTL Support** | Native | Manual | Manual |

**Why Redis?**
- Sub-millisecond reads/writes
- Native TTL for automatic session cleanup
- Horizontal scaling for production
- Perfect for conversation state

**Why Fallback to Memory?**
- Allows local development without Redis
- Tests can run without external dependencies

**Why NOT PostgreSQL?**
- Overkill for session data
- Higher latency
- More complex setup

---

## 5. Agent Design

### âœ… Chosen: **Persona + Strategy System**

**Personas**:
- Elderly Confused Person
- Young Eager Adult
- Cautious Professional

**Strategies**:
- Engagement â†’ Initial Extraction â†’ Deep Extraction â†’ Stalling â†’ Exit

**Why Personas?**
- Scammers target vulnerable people
- Realistic personas increase engagement time
- More intelligence extracted

**Why Strategies?**
- Conversation follows logical flow
- Maximizes extraction before exit
- Avoids suspicion from random behavior

**Why NOT Random Responses?**
- Scammers would disconnect
- Lower engagement metrics
- Less intelligence gathered

---

## 6. Callback Architecture

### âœ… Chosen: **Async Fire-and-Forget**

```python
asyncio.create_task(guvi_callback.send_final_result(...))
```

**Why Async?**
- API response returns immediately
- Callback happens in background
- No impact on user-facing latency

**Why NOT Synchronous?**
- Would block API response
- Risk of timeout if GUVI is slow
- Bad user experience

---

## 7. URL Threat Detection

### âœ… Chosen: **Local Blacklist + Heuristics**

**Components**:
- Known phishing domain list
- Suspicious pattern matching (IP URLs, etc.)
- URL expansion for shortened links

**Why NOT External API (VirusTotal)?**
- Rate limits on free tier
- Added latency (~200ms)
- Network dependency

---

## 8. Testing Strategy

### âœ… Chosen: **Unit + E2E + Manual**

| Type | Coverage | Files |
|------|----------|-------|
| Unit | Detection, Callback logic | `test_detection.py`, `test_callback.py` |
| E2E | Full API flow | `test_e2e.py` |
| Manual | Edge cases, UI | Swagger `/docs` |

**Why This Mix?**
- Unit tests catch logic bugs quickly
- E2E tests verify integration
- Manual tests catch UX issues

---

## 9. Deployment Target

### âœ… Chosen: **Render.com**

| Aspect | Render | Heroku | AWS |
|--------|--------|--------|-----|
| **Free Tier** | Yes | Limited | Complex |
| **Setup** | Simple | Simple | Complex |
| **Auto Deploy** | Yes | Yes | Manual |
| **Custom Domain** | Yes | Paid | Yes |

**Why Render?**
- Free tier with generous limits
- Simple GitHub integration
- Auto-deploys on push
- Perfect for hackathons

---

## Summary

Every decision was made with these priorities:
1. **Accuracy**: Multi-tier detection for 94%+ accuracy
2. **Speed**: <2s total latency
3. **Cost**: Free tier friendly
4. **Hackathon**: Fast to build, impressive to demo
